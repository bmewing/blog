<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Munging on Thug R Life</title>
    <link>http://thug-r.life/categories/data-munging/</link>
    <description>Recent content in Data Munging on Thug R Life</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 06 Aug 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://thug-r.life/categories/data-munging/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Theta Joins in R</title>
      <link>http://thug-r.life/post/2017-08-06-fuzzy-joining/</link>
      <pubDate>Sun, 06 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>http://thug-r.life/post/2017-08-06-fuzzy-joining/</guid>
      <description>&lt;div id=&#34;theta-joins&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Theta Joins&lt;/h1&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://thug-r.life/img/2017-08-06-picture1.png&#34; alt=&#34;Common Joins&#34; width=&#34;400&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Common Joins&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If you’re like me, you’re looking at that diagram wondering what the heck is a ‘theta’ join?! Well, that’s the formal name for a ‘fuzzy’ join. In fact, you can read about all the different join types &lt;a href=&#34;https://stackoverflow.com/questions/7870155/difference-between-a-theta-join-equijoin-and-natural-join&#34;&gt;in this StackOverflow answer&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;An equi join is the most common kind of join because it leverages keys in your data source which are essential in a SQL environment. In SQL it would be something like:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;select a.*, b.*&lt;br /&gt;
from dbo.table1 as a&lt;br /&gt;
left join dbo.table2 as b&lt;br /&gt;
on a.key = b.key&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Actually this is a ‘natural’ join because the key has the same name in both sources and I’m using an equality to establish the join. But, because I’m joining based on two keys being the same, this fits into the ‘equi’ join category.&lt;/p&gt;
&lt;p&gt;How can we do ‘equi’ joins in R? I’ll present three methods below using three different packages, &lt;code&gt;data.table&lt;/code&gt;, &lt;code&gt;dplyr&lt;/code&gt;, and &lt;code&gt;sqldf&lt;/code&gt;.&lt;/p&gt;
&lt;div id=&#34;getting-ready&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Getting ready&lt;/h2&gt;
&lt;p&gt;We’ll need to load some packages. Please note the versions I’m using. Also, the ‘live’ code I’m running is on my 64-bit Windows 7 laptop (8Gb RAM, i7-6600U CPU) in R 3.4.1&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Package&lt;/th&gt;
&lt;th&gt;Version&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;data.table&lt;/td&gt;
&lt;td&gt;1.10.4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;sqldf&lt;/td&gt;
&lt;td&gt;0.4-10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;dplyr&lt;/td&gt;
&lt;td&gt;0.5.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(data.table)
library(sqldf)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;equijoin-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;equijoin data&lt;/h3&gt;
&lt;p&gt;Two quick tables to demonstrate equijoin behavior in each ecosystem. Here we have some customer id’s in two tables, one which has purchases (df1) and another with their location. I’ll write everything in a function to enable easy benchmarking and profiling.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df1 = data.frame(CustomerId = c(1:6), Product = c(rep(&amp;quot;Laptop&amp;quot;, 2), rep(&amp;quot;Tablet&amp;quot;, 2), rep(&amp;quot;Smartphone&amp;quot;, 2)))
df2 = data.frame(CustomerId = c(2,3,4,6), State = c(rep(&amp;quot;Tennessee&amp;quot;, 3), rep(&amp;quot;North Carolina&amp;quot;, 1)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;different-approaches-for-equijoins&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Different Approaches for Equijoins&lt;/h2&gt;
&lt;div id=&#34;base-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Base R&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baseR_left = function(){
  merge(x = df1, y = df2, by = &amp;quot;CustomerId&amp;quot;, all.x = TRUE)
}
baseR_left()
##   CustomerId    Product          State
## 1          1     Laptop           &amp;lt;NA&amp;gt;
## 2          2     Laptop      Tennessee
## 3          3     Tablet      Tennessee
## 4          4     Tablet      Tennessee
## 5          5 Smartphone           &amp;lt;NA&amp;gt;
## 6          6 Smartphone North Carolina&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data.table&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;data.table&lt;/h3&gt;
&lt;p&gt;Are you not familiar with &lt;code&gt;data.table&lt;/code&gt;? I’ll be honest, I don’t use it a lot but it’s described as follows on CRAN:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Fast aggregation of large data (e.g. 100GB in RAM), fast ordered joins, fast add/modify/delete of columns by group using no copies at all, list columns, a fast friendly file reader and parallel file writer. Offers a natural and flexible syntax, for faster development.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The syntax is different from a lot of what I’m used to which is why I don’t use it a bunch, but that’s a bad reason to not consider it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dt1 = data.table(df1)
dt2 = data.table(df2)
#left join
dt_left = function(){
  dt2[dt1,on=.(CustomerId = CustomerId)]
}
dt_left()
##    CustomerId          State    Product
## 1:          1             NA     Laptop
## 2:          2      Tennessee     Laptop
## 3:          3      Tennessee     Tablet
## 4:          4      Tennessee     Tablet
## 5:          5             NA Smartphone
## 6:          6 North Carolina Smartphone&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I think this sytax is a bit backwards since Y[X] produces a left join of Y onto X meaning you put the left anchors on the right… anyway.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sqldf&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;sqldf&lt;/h3&gt;
&lt;p&gt;Are you a lover of SQL and maybe new to R? I have a coworker who lived and breathed SQL for decades before learning R and they swear by this package to manage data. It literally sets up a database in the background to support the SQL language. That’s &lt;em&gt;dedication&lt;/em&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The sqldf() function is typically passed a single argument which is an SQL select statement where the table names are ordinary R data frame names. sqldf() transparently sets up a database, imports the data frames into that database, performs the SQL select or other statement and returns the result using a heuristic to determine which class to assign to each column of the returned data frame. The sqldf() or read.csv.sql() functions can also be used to read filtered files into R even if the original files are larger than R itself can handle. ‘RSQLite’, ‘RH2’, ‘RMySQL’ and ‘RPostgreSQL’ backends are supported.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqldf_left = function(){
  sqldf(&amp;quot;SELECT a.CustomerId, Product, State 
         FROM df1 a
         LEFT JOIN df2 b 
           on a.CustomerID = b.CustomerID&amp;quot;)
}
sqldf_left()
## Loading required package: tcltk
## Warning: Quoted identifiers should have class SQL, use DBI::SQL() if the
## caller performs the quoting.
##   CustomerId    Product          State
## 1          1     Laptop           &amp;lt;NA&amp;gt;
## 2          2     Laptop      Tennessee
## 3          3     Tablet      Tennessee
## 4          4     Tablet      Tennessee
## 5          5 Smartphone           &amp;lt;NA&amp;gt;
## 6          6 Smartphone North Carolina&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;dplyr&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;dplyr&lt;/h3&gt;
&lt;p&gt;If I described myself as having a man crush on Hadley Wickham it would be accurate. I have a signed copy of his book ‘Advanced R’ on my bookshelf which I treasure. I didn’t even get it signed, I sent it with a friend who went to an R conference where I knew Hadley would be present. ANYWAY… dplyr is part of the &lt;a href=&#34;https://www.tidyverse.org/&#34;&gt;tidyverse&lt;/a&gt; which is a collection of packages to support ‘tidy’ data analysis. It includes support for many operations include joins.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dplyr_left = function(){
  left_join(df1,df2,by=&amp;quot;CustomerId&amp;quot;)
}
dplyr_left()
##   CustomerId    Product          State
## 1          1     Laptop           &amp;lt;NA&amp;gt;
## 2          2     Laptop      Tennessee
## 3          3     Tablet      Tennessee
## 4          4     Tablet      Tennessee
## 5          5 Smartphone           &amp;lt;NA&amp;gt;
## 6          6 Smartphone North Carolina&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;profiling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Profiling&lt;/h2&gt;
&lt;p&gt;Let’s profile these just to get a sense of where they stand.&lt;/p&gt;
&lt;div id=&#34;profmem&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;profmem&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Ecosystem&lt;/th&gt;
&lt;th&gt;profmem bytes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Base R&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;data.table&lt;/td&gt;
&lt;td&gt;58,048&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;sqldf&lt;/td&gt;
&lt;td&gt;183,888&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;dplyr&lt;/td&gt;
&lt;td&gt;5,088&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;microbenchmark&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;microbenchmark&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(microbenchmark)
microbenchmark(base = baseR_left(),
               dt = dt_left(),
               sqldf = sqldf_left(),
               dplyr = dplyr_left(),
               times = 100)
## Unit: microseconds
##   expr       min        lq       mean    median        uq       max neval
##   base   464.933   553.179   609.7296   600.766   645.072  1082.290   100
##     dt   643.613   767.048   929.3189   882.461   998.785  2450.104   100
##  sqldf 12769.785 13586.243 14985.2887 14336.698 15653.096 25150.483   100
##  dplyr   277.502   348.791   435.1666   390.544   431.750  1904.583   100&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;results&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Results&lt;/h4&gt;
&lt;p&gt;Base R has the smallest memory footprint (duh) and performs pretty well. &lt;code&gt;merge&lt;/code&gt; is pretty flexible and can do some cool stuff. dplyr has the fastest performance on this toy set and a small memory footprint. data.table takes about twice as long as dplyr and is ~10x the memory footprint. sqldf is slow and bloated - but what else could you expect from a package that is literally spinning up a database in the background? It’s the price you pay to use SQL in R.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;different-approaches-for-theta-joins&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Different Approaches for Theta Joins&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;the-small-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The (small) data&lt;/h2&gt;
&lt;p&gt;We’ll use the same data for the toy problem and the more realistic version. This is the result of some text mining, specifically, named entity extraction from corpuses of text. It’s a two part, non-sequential process. We identify sentences by extracting the starting and ending characters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sent_df = read.table(text=&amp;quot;id start end
                            1     1  39
                            2    42  63&amp;quot;, header=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we extract entities by getting the type, starting and ending characters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ent_df = read.table(text=&amp;quot;start end     ent_type
                              1   9 organization
                             23  31        money
                             45  50 organization&amp;quot;, header=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But, to help our users get the context of where entities appear as well as to get co-occurence information, we need to know what sentence the entity appeared in. Additionally, we only want to keep the sentence id and the entity type.&lt;/p&gt;
&lt;p&gt;In this case, we can see the first entity (an organization) starts at location 1 and ends at 9 which puts it in sentence 1 (which starts at location 1 and ends at 39). The second entity (money!) is also in sentence 1 (starts at 23 and ends at 31) while the last entity belongs in sentence 2. An equi join won’t work because I need to check if entity.start &amp;gt;= sentence.start AND entity.end &amp;lt;= sentence.end. Luckily, we have a few approaches.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;base-r-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Base R&lt;/h2&gt;
&lt;p&gt;What we’ll do is cross join the data together, then do some filtering. I’m doing this all with base R functionality which I’m not totally used to doing, so this may not be the most perfect form of filtering. I decided to try using the &lt;code&gt;%between%&lt;/code&gt; operator as well. Since an entity cannot be extracted which spans sentences all we really need to check is that the entity start is between the sentence start and end.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;baseR_fuzzy = function(){
  tmp = merge(x = sent_df, y = ent_df, by = NULL)
  tmp[tmp$start.x &amp;lt;= tmp$start.y &amp;amp; tmp$end.x &amp;gt;= tmp$end.y,c(&amp;quot;id&amp;quot;,&amp;quot;ent_type&amp;quot;)]
}

baseR_fuzzy_between = function(){
  tmp = merge(x = sent_df, y = ent_df, by = NULL)
  tmp[tmp$start.y %between% tmp[,c(&amp;quot;start.x&amp;quot;,&amp;quot;end.x&amp;quot;)],c(&amp;quot;id&amp;quot;,&amp;quot;ent_type&amp;quot;)]
}
  
baseR_fuzzy()
##   id     ent_type
## 1  1 organization
## 3  1        money
## 6  2 organization&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data.table-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;data.table&lt;/h2&gt;
&lt;p&gt;One great thing about data.table is that it supports fuzzy joins right out of the gate. However, there’s a lot of ambiguity in the join logic. Because doing a left join takes Y[X] and we’re joining the entities onto the sentences we list column names in the left-to-right order, not the left join order. So, where we have &lt;code&gt;on=.(start &amp;gt;= start)&lt;/code&gt; you can think of it as &lt;code&gt;on=.(start.Y &amp;gt;= start.X)&lt;/code&gt; which is equivalent to &lt;code&gt;on=.(start.ent &amp;gt;= start.sent)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sent_dt = data.table(sent_df)
ent_dt  = data.table(ent_df)

dt_fuzzy = function(){
  ent_dt[sent_dt, on=.(start &amp;gt;= start, end &amp;lt;= end)][,.(id, ent_type)]
}
dt_fuzzy()
##    id     ent_type
## 1:  1 organization
## 2:  1        money
## 3:  2 organization&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sqldf-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;sqldf&lt;/h2&gt;
&lt;p&gt;sqldf also has the &lt;code&gt;BETWEEN&lt;/code&gt; operator which lets us simplify the inequality of our join.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqldf_fuzzy = function() {
  sqldf(
      &amp;quot;
      SELECT id, ent_type
      FROM sent_df a, ent_df b
      WHERE b.start BETWEEN a.start AND a.end 
      &amp;quot;
  )
}
sqldf_fuzzy()
##   id     ent_type
## 1  1 organization
## 2  1        money
## 3  2 organization&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fuzzyjoin&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;fuzzyjoin&lt;/h2&gt;
&lt;p&gt;Here I’ll introduce a new package aptly named &lt;code&gt;fuzzyjoin&lt;/code&gt; (I’m using version 0.1.2). This adds ‘fuzzy’ versions of all the &lt;code&gt;dplyr&lt;/code&gt; joins and is fully compatible in the &lt;code&gt;tidyverse&lt;/code&gt;. In addition to providing both tables we need to provide match functions which provide logical indicators of fuzzy matching. What’s interesting is that each row is returned twice which must be a function of providing two match functions.&lt;/p&gt;
&lt;p&gt;This is also the first time I use the &lt;code&gt;magrittr&lt;/code&gt; pipes (%&amp;gt;%). If you’re not familiar with them I’d suggest learning about them because they’re great and I have stickers of them on my truck and motorcycle.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(fuzzyjoin)
startMatch = function(x,y){
  x &amp;lt;= y
}
endMatch = function(x,y){
  x &amp;gt;= y
}
fuzzy_fuzzy = function(){
  fuzzy_join(sent_df,ent_df,by=c(&amp;quot;start&amp;quot;,&amp;quot;end&amp;quot;),match_fun = list(start = startMatch,end = endMatch)) %&amp;gt;% 
    select(id,ent_type) %&amp;gt;% 
    distinct()
}
fuzzy_fuzzy()
##   id     ent_type
## 1  1 organization
## 2  1        money
## 3  2 organization&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pure-tidyverse&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pure tidyverse&lt;/h2&gt;
&lt;p&gt;I’m a fan of limiting the number of packages your code relies upon. Adding &lt;code&gt;fuzzyjoin&lt;/code&gt; to handle this problem feels unnecessary (it provides great value in other applications because of the flexibility of the match functions) so I’d like to try and avoid it but also leverage the tidyverse. So I’m going to load the &lt;code&gt;tidyr&lt;/code&gt; package (I’m using version 0.6.1) which is a solid compliment to &lt;code&gt;dplyr&lt;/code&gt; and does lots of useful stuff. I know it looks like I’m just swapping out one package for another, but given the general practicality of tidyr compared to fuzzyjoin, I think it’s OK.&lt;/p&gt;
&lt;p&gt;tidyr adds a cross_join function (not present in dplyr!!) which has a funky problem of not adjusting the names of duplicate columns. This impacts the ability of dplyr and other things to work on the resulting tibble (data.frame). So, I manually adjust the names using &lt;code&gt;names&amp;lt;-&lt;/code&gt;, filter down to what I want and then select the columns. The approach is identical to the base R code, just using the tidyverse.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyr)
tidy_fuzzy = function(){
  sent_df %&amp;gt;% 
    crossing(ent_df) %&amp;gt;% 
    `names&amp;lt;-`(c(&amp;quot;id&amp;quot;,&amp;quot;start.x&amp;quot;,&amp;quot;end.x&amp;quot;,&amp;quot;start.y&amp;quot;,&amp;quot;end.y&amp;quot;,&amp;quot;ent_type&amp;quot;)) %&amp;gt;% 
    filter(end.x &amp;gt;= end.y &amp;amp; start.x &amp;lt;= start.y) %&amp;gt;% 
    select(id,ent_type)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;profiling-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Profiling&lt;/h2&gt;
&lt;p&gt;Let’s profile these (small) fuzzy joins just to get a sense of where they stand.&lt;/p&gt;
&lt;div id=&#34;profmem-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;profmem&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Ecosystem&lt;/th&gt;
&lt;th&gt;profmem bytes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Base R&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Base R (%between%)&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;data.table&lt;/td&gt;
&lt;td&gt;83,136&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;sqldf&lt;/td&gt;
&lt;td&gt;184,480&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;fuzzyjoin&lt;/td&gt;
&lt;td&gt;117,104&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;tidyverse&lt;/td&gt;
&lt;td&gt;41,848&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;microbenchmark-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;microbenchmark&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;microbenchmark(base = baseR_fuzzy(),
               base_between = baseR_fuzzy_between(),
               dt = dt_fuzzy(),
               sqldf = sqldf_fuzzy(),
               fuzzyjoin = fuzzy_fuzzy(),
               tidyverse = tidy_fuzzy(),
               times = 100)
## Unit: microseconds
##          expr       min        lq       mean     median         uq
##          base   453.264   517.625   639.8173   558.2845   645.4365
##  base_between   442.325   530.753   739.3164   577.0640   712.3500
##            dt  1244.926  1472.652  1676.3653  1555.2455  1789.7180
##         sqldf 12932.784 13983.532 15989.1457 15515.4395 17327.5825
##     fuzzyjoin 12630.122 13841.500 16087.9558 15828.6760 17627.5100
##     tidyverse  1314.575  1480.492  1977.8531  1586.4235  1876.5050
##        max neval
##   1973.503   100
##   6578.706   100
##   3068.556   100
##  24026.988   100
##  22814.882   100
##  21330.379   100&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;results-1&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Results&lt;/h4&gt;
&lt;p&gt;Both base R functions have no memory footprint and actually run the fastest with the &lt;code&gt;%between%&lt;/code&gt; function being a touch slower but not enough to ignore it’s improved readability. &lt;code&gt;fuzzyjoin&lt;/code&gt; has a large footprint and runs the slowest being in the same ballpark as &lt;code&gt;sqldf&lt;/code&gt; - neither of these are great choices for the task we have. &lt;code&gt;data.table&lt;/code&gt; and &lt;code&gt;tidyverse&lt;/code&gt; are in the same ballpark, but are substantially slower than base R. BUT this was a very small toy problem. Let’s do something more meaningful.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;bigger-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bigger Data&lt;/h2&gt;
&lt;p&gt;It turned out that in the real-world case we only needed to do fuzzy joining on a per-document basis which meant the data never got so big that scaling issues of each approach would matter. It was scaling how much parallel processing we could do to slam documents through.&lt;/p&gt;
&lt;p&gt;But that’s boring. We have scaling issues we need to test and solve.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;http://thug-r.life/img/2017-08-06-picture2.png&#34; alt=&#34;Common Joins&#34; width=&#34;400&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Common Joins&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Originally I created 5000 sentences with ~23,000 entities, but this proved to be too much for some of the functions to handle on my laptop (having closed all other applications first!). So, I shrunk it to 2000 sentences and ~9,000 entities.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(2300)
nsent = 2000
sentence_length = floor(rnorm(nsent,100,10))
sent_df = data.frame(id = 1:nsent,
                     start = Reduce(sum,sentence_length,init=1,accumulate = T)[1:nsent],
                     end = cumsum(sentence_length))
stop = FALSE
i = 1
ent_df = data.frame(start = NA, end = NA, ent_type = NA)
while(!stop){
  ent_start = as.logical(rbinom(n=1,size=1,prob = 0.2))
  if(ent_start){
    sent = max(which(sent_df$start &amp;lt;= i))
    sent_end = sent_df$end[sent_df$id == sent]
    ent_end = min(c(i+rpois(1,1)+3,sent_end))
    if(ent_end-i &amp;lt; 3){
      i = i+3
      next
    }
    ent_type = sample(c(&amp;quot;organization&amp;quot;,&amp;quot;person&amp;quot;,&amp;quot;money&amp;quot;),size = 1)
    ent_df = rbind(ent_df,data.frame(start=i, end=ent_end, ent_type=ent_type))
    i = ent_end+2
  }
  i = i+3
  if(i &amp;gt; max(sent_df$end)) stop = TRUE
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we have a bunch of data - so let’s test stuff!&lt;/p&gt;
&lt;div id=&#34;microbenchmark-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;microbenchmark&lt;/h3&gt;
&lt;p&gt;I suspect some of these are going to run very poorly, so we’ll start with an n of 1. This isn’t a good benchmark but when you see the eval times, you’ll realize why.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;microbenchmark(base = baseR_fuzzy(),
               base_between = baseR_fuzzy_between(),
               dt = dt_fuzzy(),
               sqldf = sqldf_fuzzy(),
               fuzzyjoin = fuzzy_fuzzy(),
               tidyverse = tidy_fuzzy(),
               times = 1)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;Eval Time (ms)&lt;/th&gt;
&lt;th&gt;Scale Up&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Base&lt;/td&gt;
&lt;td&gt;42,564&lt;/td&gt;
&lt;td&gt;67,669&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Base (%between%)&lt;/td&gt;
&lt;td&gt;53,239&lt;/td&gt;
&lt;td&gt;81,280x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;data.table&lt;/td&gt;
&lt;td&gt;17&lt;/td&gt;
&lt;td&gt;9x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;sqldf&lt;/td&gt;
&lt;td&gt;1,296&lt;/td&gt;
&lt;td&gt;798x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;fuzzyjoin&lt;/td&gt;
&lt;td&gt;355,983&lt;/td&gt;
&lt;td&gt;16,100x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;tidyverse&lt;/td&gt;
&lt;td&gt;65,113&lt;/td&gt;
&lt;td&gt;38,301x&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The ‘scale up’ column is simply dividing the execution time of the 1000x sentence (3000x entities) data by the execution time of the original data. I find the results to be somewhat surprising.&lt;/p&gt;
&lt;div id=&#34;results-2&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Results&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;data.table&lt;/code&gt; is the clear winner overall - it was middle of the pack on the toy data and scales better than the data.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sqldf&lt;/code&gt; has a lot of overhead and ran (relatively) poorly on the toy problem but scales very well with the larger data.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;fuzzyjoin&lt;/code&gt; took nearly 6 minutes to run where the next slowest was closer to a minute. This is not a good choice for the type of fuzzy joining we’re doing.&lt;/li&gt;
&lt;li&gt;Base R (both versions) and &lt;code&gt;tidyverse&lt;/code&gt; were in the same rough ballpark, ~1 minute with ridiculous scaling problems.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I’ve shown you 6 different approaches to doing fuzzy joins (aka theta joins) and demonstrated their relative merits. We’ve seen that performance on small joins is no indication of performance on large joins but &lt;code&gt;data.table&lt;/code&gt; scales very well on a variety of sizes and &lt;code&gt;sqldf&lt;/code&gt; leverages it’s bloat in small sizes to perform well on larger sizes. Also important to note - base R has reasonable performance and beats a ‘tidy’ solution at the (potential) cost of readability.&lt;/p&gt;
&lt;p&gt;Should you pick up &lt;code&gt;data.table&lt;/code&gt; and replace all your tidy work with it? I’m begining to believe that maybe I should. &lt;code&gt;data.table&lt;/code&gt; has at least one other practical advantage over the tidyverse. Stability. &lt;code&gt;data.table&lt;/code&gt; is past version 1.0 where &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;tidyr&lt;/code&gt; are not. This is especially apparent with the fairly frequent breaking changes made to the tidyverse which can have a strong, negative impact in an enterprise environment.&lt;/p&gt;
&lt;p&gt;Well - I hope you have fun with theta joins (and start calling them that!)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>