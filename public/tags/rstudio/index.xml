<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rstudio on Thug R Life</title>
    <link>http://thug-r.life/tags/rstudio/</link>
    <description>Recent content in Rstudio on Thug R Life</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Aug 2017 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="http://thug-r.life/tags/rstudio/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>purrr Tricks with All Subset Regression</title>
      <link>http://thug-r.life/post/2017-08-24-purrr-tricks-with-all-subset-regression/</link>
      <pubDate>Thu, 24 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>http://thug-r.life/post/2017-08-24-purrr-tricks-with-all-subset-regression/</guid>
      <description>All Subsets RegressionWhat is all subsets regression? It’s a technique for model building which involves taking a set of independent variables \(X_1..i\) and regressing them in sets of \(k\), where \(k\) is in \(\{1,2,\dots,i\}\), against the response variable \(Y\). The ‘all’ part of ‘all subsets’ means it’s every combination of \(X_{1..i}\) being drawn \(k\) at a time.
But…why?You’re probably familiar with forward, backward or stepwise model building where terms are added (or removed) from a model one at a time while attempting to maximize or minimize some ‘goodness’ criteria.</description>
      <content:encoded><div id="all-subsets-regression" class="section level2">
<h2>All Subsets Regression</h2>
<p>What is all subsets regression? It’s a technique for model building which involves taking a set of independent variables <span class="math inline">\(X_1..i\)</span> and regressing them in sets of <span class="math inline">\(k\)</span>, where <span class="math inline">\(k\)</span> is in <span class="math inline">\(\{1,2,\dots,i\}\)</span>, against the response variable <span class="math inline">\(Y\)</span>. The ‘all’ part of ‘all subsets’ means it’s every combination of <span class="math inline">\(X_{1..i}\)</span> being drawn <span class="math inline">\(k\)</span> at a time.</p>
<div id="butwhy" class="section level3">
<h3>But…why?</h3>
<p>You’re probably familiar with forward, backward or stepwise model building where terms are added (or removed) from a model one at a time while attempting to maximize or minimize some ‘goodness’ criteria. These are generally regarded as bad - so why all subsets? Really, it has all the faults of the other methods but it has at least one advantage - you can have more complex ‘goodness’ criteria which may hit local maxima/minima (interupting the previous methods) but allow you to find the global. So, it’s fishing to the max.</p>
</div>
<div id="leaps" class="section level3">
<h3>leaps</h3>
<p>R has a great package called <code>leaps</code> which implements all subsets regression. But…it doesn’t meet my needs.</p>
</div>
</div>
<div id="all-subsets-regression-and-missing-values" class="section level2">
<h2>All Subsets Regression and Missing Values</h2>
<p>Let’s consider a dataset which has missing values. I’ll generate a dummy set of data containing 4 independent variables and one dependent which depends on the first three variables. Then, I’ll randomly assign ~12% of the data to be missing.</p>
<pre class="r"><code>set.seed(1001)
X1 = c(-1,0,1)
df = expand.grid(X1=X1,X2=X1,X3=X1,X4=X1)
df$Y = df$X1+3*df$X2-0.5*df$X3+rnorm(81,0,1)
df$X1[sample(1:81,10,replace = FALSE)] = NA
df$X2[sample(1:81,10,replace = FALSE)] = NA
df$X3[sample(1:81,10,replace = FALSE)] = NA
df$X4[sample(1:81,10,replace = FALSE)] = NA</code></pre>
<p>We can verify that no rows are completely missing:</p>
<pre class="r"><code>any(apply(is.na(df[,1:4]),1,all))
## [1] FALSE
# Explanation
# df %&gt;% 
#   #only consider the first 4 columns
#   .[,1:4] %&gt;% 
#   #convert to a logical data frame checking for missing values
#   is.na() %&gt;% 
#   #this function applies another function row or column wise to the input dataframe or matrix
#   apply(
#     # 1 means rows
#     1, 
#     # Check if all the values are TRUE
#     all) %&gt;% 
#   # now we have a vector of logicals checking if the whole row of df was missing
#   # check if any are TRUE
#   any()</code></pre>
<p>And we can count that there are 50 complete rows (just 62% of the original data).</p>
<pre class="r"><code>sum(apply(!is.na(df[,1:4]),1,all))
## [1] 50
# Explanation
# df %&gt;% 
#   #only consider the first 4 columns
#   .[,1:4] %&gt;% 
#    #convert to a logical data frame checking for missing values
#   is.na() %&gt;%
#   #flip the logical values
#   `!` %&gt;% 
#   #this function applies another function row or column wise to the input dataframe or matrix
#   apply(
#     # 1 means rows
#     1,
#     # Check if all the values are TRUE
#     all
#   ) %&gt;% 
#   # now we have a vector of logicals checking if the whole row of df is present
#   # sum this up to get a count
#   sum()</code></pre>
<p>In linear regression observations cannot be included if they are missing because we don’t know what value they take. And sure, there’s missing value imputation - as always in statistics, if you’re willing to make assumptions whatever you’re doing becomes more statistically powerful, but also more real life fragile. With missing value imputation you have to make some assumptions about why they’re missing and then you have to generate the values themselves which requires another set of assumptions. It’s fine to not want to make those assumptions.</p>
<p>So despite the fact that we have 81 rows, we only have 50 rows for fitting a linear regression model which means if we fit the full model we’ll only have <span class="math inline">\(n-1-k=50-1-4=45\)</span> degrees of freedom.</p>
<pre class="r"><code>fullModel = lm(Y~X1+X2+X3+X4,data=df)
summary(fullModel)
## 
## Call:
## lm(formula = Y ~ X1 + X2 + X3 + X4, data = df)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.4627 -0.7666 -0.0156  0.6580  3.4490 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.02660    0.18046  -0.147   0.8835    
## X1           0.98850    0.22582   4.377 7.07e-05 ***
## X2           2.96688    0.22758  13.036  &lt; 2e-16 ***
## X3          -0.53690    0.22246  -2.413   0.0199 *  
## X4           0.06567    0.21863   0.300   0.7653    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.249 on 45 degrees of freedom
##   (31 observations deleted due to missingness)
## Multiple R-squared:  0.8074, Adjusted R-squared:  0.7902 
## F-statistic: 47.15 on 4 and 45 DF,  p-value: 1.547e-15</code></pre>
<p>You can see the DF agree with what we expect to see. Also, please ignore the fact that we’re getting results which are nearly perfect. It’s hard to generate dummy data that looks like real world data.</p>
<div id="first-subset" class="section level3">
<h3>First Subset</h3>
<p>The first subset is typically just the first indpendent variable we encounter, on it’s own, so X1 in our case, but we’ll start with X1+X2 (because it helps with comparing to leaps). We have 61 complete observations across X1 and X2.</p>
<pre class="r"><code>sum(apply(!is.na(df[,1:2]),1,all))
## [1] 61</code></pre>
<p>So when we fit the model we expect to have <span class="math inline">\(n-1-k=61-1-2=58\)</span> degrees of freedom.</p>
<pre class="r"><code>firstSubset = lm(Y~X1+X2,data=df)
summary(firstSubset)
## 
## Call:
## lm(formula = Y ~ X1 + X2, data = df)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.64333 -0.79388 -0.01199  0.74753  2.85868 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.05194    0.16760   0.310    0.758    
## X1           0.87991    0.20895   4.211 8.96e-05 ***
## X2           2.88003    0.21237  13.561  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.304 on 58 degrees of freedom
##   (20 observations deleted due to missingness)
## Multiple R-squared:  0.7794, Adjusted R-squared:  0.7718 
## F-statistic: 102.5 on 2 and 58 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>This is a pretty good model, and we nailed basic arithmetic! How does this model compare to what we would get if we used only the complete observations?</p>
<pre class="r"><code>firstSubset_limited = lm(Y~X1+X2,data=fullModel$model)
summary(firstSubset_limited)
## 
## Call:
## lm(formula = Y ~ X1 + X2, data = fullModel$model)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.4684 -0.8767 -0.0286  0.7977  3.0501 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.09628    0.18551  -0.519 0.606175    
## X1           0.94976    0.23369   4.064 0.000182 ***
## X2           2.90673    0.23552  12.342 2.37e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.301 on 47 degrees of freedom
## Multiple R-squared:  0.7819, Adjusted R-squared:  0.7726 
## F-statistic: 84.26 on 2 and 47 DF,  p-value: 2.869e-16</code></pre>
<p>It’s not very different, but the R^2 value is a little higher. Also the AIC and BIC are little better for the reduced data set.</p>
<pre class="r"><code>cat(&quot;All available observations AIC:    &quot;,AIC(firstSubset),&quot;\nJust the complete observations AIC:&quot;,AIC(firstSubset_limited),&quot;\n\nAll available observations BIC:    &quot;,BIC(firstSubset),&quot;\nJust the complete observations BIC:&quot;,BIC(firstSubset_limited))
## All available observations AIC:     210.4281 
## Just the complete observations AIC: 173.0897 
## 
## All available observations BIC:     218.8716 
## Just the complete observations BIC: 180.7378</code></pre>
</div>
<div id="leaps-1" class="section level3">
<h3>leaps</h3>
<p>So what does leaps produce? The <code>regsubsets</code> lets us specify a formula just like in lm and then it fits all subsets of the specified full model. The summary shows the best model for each set size <span class="math inline">\(k\)</span>. It selected X2 in the <span class="math inline">\(k=1\)</span> case, X1+X2 in the <span class="math inline">\(k=2\)</span> case, X1+X2+X3 in the <span class="math inline">\(k=3\)</span> and the full model for <span class="math inline">\(k=4\)</span>.</p>
<pre class="r"><code>library(leaps) #Version 3.0
leapsResults = leaps::regsubsets(Y~X1+X2+X3+X4,data=df)
summary(leapsResults)
## Subset selection object
## Call: regsubsets.formula(Y ~ X1 + X2 + X3 + X4, data = df)
## 4 Variables  (and intercept)
##    Forced in Forced out
## X1     FALSE      FALSE
## X2     FALSE      FALSE
## X3     FALSE      FALSE
## X4     FALSE      FALSE
## 1 subsets of each size up to 4
## Selection Algorithm: exhaustive
##          X1  X2  X3  X4 
## 1  ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot;
## 2  ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot;
## 3  ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot;
## 4  ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot;</code></pre>
<p>The summary object has a lot in it including BIC, <span class="math inline">\(R^2_{Adj}\)</span>, Mallows Cp and more. It’s important to note that <code>leaps</code> only retains information about the best models selected.</p>
<p>It looks like the way <code>stats::BIC</code> calculates BIC and the way <code>leaps</code> does don’t match up. That won’t be much use. But look at the second value of <span class="math inline">\(R^2_{Adj} = 0.7726\)</span> - that’s what we calculated when using only the complete observations across all variables.</p>
<pre class="r"><code>leapsSummary = summary(leapsResults)
leapsSummary$bic[2] #second value for the two term model
## [1] -64.40754
leapsSummary$adjr2[2]
## [1] 0.7726352</code></pre>
</div>
</div>
<div id="how-much-data-to-use" class="section level2">
<h2>How much data to use?</h2>
<p>Using only the complete observations in all subset regression is common across several platforms including R and JMP. But why? Is it more statistically valid to ignore data that could be part of the regression model? Or is it simply more computationally efficient? It seems to me that each model you fit should be fit using all data available to it - why should my results be different because I collected results on X3 and X4? I should get the exact same results when fitting the selected model as when doing my model building. So, how can we do all subsets regression using all the data available? And when do we get to purrr?</p>
</div>
<div id="new-functions" class="section level2">
<h2>New Functions</h2>
<p>Below I present 3 functions with a limited amount of safe programming tossed in.</p>
<div id="allsubsetsregression" class="section level3">
<h3>allSubsetsRegression</h3>
<p>The main function is <code>allSubsetsRegression</code> which is too long of a name. It accepts * a data frame which should contain only the independent variables to be included and the dependent variable * a quoted string for the column name in <code>data</code> which is the independent variable * optionally, minimum number of variables to include * optionally, maximum number of variables to include (useful if your data is wider than it is tall)</p>
<p>This makes a call to <code>calcKreg</code></p>
<pre class="r"><code>allSubsetsRegression = function(data,y,minV=1L,maxV){
  if(missing(maxV)) maxV = as.integer(ncol(data)-1)
  if(minV &gt; maxV) stop(&quot;Max number of variables must be bigger than the min number of variables&quot;)
  if(!is.integer(minV) | !is.integer(maxV)) stop(&quot;Min/Max variables must be integers&quot;)
  if(!is.data.frame(data)) stop(&quot;Data must be provided in a data frame&quot;)
  if(!is.character(y)) stop(&quot;Response variable &#39;y&#39; must be provided as a quoted string&quot;)
  xnames = names(data)[names(data) != y]
  output = lapply(minV:maxV,calcKreg,data=data,y=y,xnames=xnames)
  class(output) = c(class(output),&quot;subReg&quot;)
  return(output)
}</code></pre>
</div>
<div id="calckreg" class="section level3">
<h3>calcKreg</h3>
<p>The name looks like “calc Kreg” which sounds like a weird German thing but it’s actually “calc K reg” meaning “calculate the <span class="math inline">\(k^{th}\)</span> regression set”. This requires * the provided data * the quoted string of the dependent variable * the vector of quoted strings of the independent variables * k - how big the set is</p>
<p>This leverages the built in function <code>combn</code> which returns a matrix of all combinations of <span class="math inline">\(X_i\)</span> of size <span class="math inline">\(k\)</span>. This in turn calls the function <code>genReg</code>.</p>
<pre class="r"><code>calcKreg = function(data,y,xnames,k){
  runs = combn(xnames, k)
  models = lapply(1:ncol(runs), genReg, data=data, y=y, runs=runs)
  names(models) = apply(runs, 2, paste, collapse=&#39;+&#39;)
  return(models)
}</code></pre>
</div>
<div id="genreg" class="section level3">
<h3>genReg</h3>
<p><code>genReg</code> (“generate regression”) requires * the provided data * the quoted string of the dependent variable * the output of <code>combn</code> * the column being used this run</p>
<p>It generates a formula by pasting the needed independent variables together with “+” and then returns the full lm object.</p>
<pre class="r"><code>genReg = function(data,y,runs,i){
  x = runs[,i]
  form = paste0(y,&quot;~&quot;,paste(x,collapse=&quot;+&quot;))
  return(lm(form,data=data))
}</code></pre>
<div id="danger-will-robinson-danger" class="section level4">
<h4>DANGER, WILL ROBINSON! DANGER!</h4>
<p>I’m returning the <em>entire</em> lm object? Well, yeah. If you have stupid big data or stupid wide data, this won’t work well, but also, you probably aren’t really wanting to do linear regression. I’m just guessing. The advantage of returning the whole object is it will let me use whatever ‘goodness’ measure I after the fact - even use several, to determine which model I want to keep.</p>
</div>
</div>
</div>
<div id="purrr" class="section level2">
<h2>purrr</h2>
<p><code>purrr</code> is a package from the tidyverse that provides a set of tools to improve functional programming. It has consistent syntax versions of <code>map</code> and <code>reduce</code> type functions that make code more readable than the base versions in R. But you’ll notice I used <code>lapply</code> above instead of <code>purrr::map</code>. I’m used to using <code>lapply</code> and I don’t like loading a whole package for simple use cases like that. Where I do like using it is for more complex cases - like <code>at_depth</code></p>
<div id="at_depth" class="section level3">
<h3>at_depth</h3>
<p><code>lapply</code> takes an object and applies a function to each element of the object and returns it in a list. Consider a heirarchical list like the one below.</p>
<pre class="r"><code>dumbList = list(z=list(a=c(1,2), b=c(2,3)), y=list(a=c(1,1),b=c(1,4)), x=list(a=c(2,2),b=c(2,0)))</code></pre>
<p>Maybe I want to sum each element so that z<span class="math inline">\(a = 3, y\)</span>a = 2, x$a = 4, etc…</p>
<pre class="r"><code>lapply(dumbList,sum)
## Error in FUN(X[[i]], ...): invalid &#39;type&#39; (list) of argument</code></pre>
<p>It doesn’t work because <code>dumbList[[1]]</code> is a list and you can’t sum a list. However, with <code>purrr::at_depth</code> we can specify the depth we want to work at - in this case, the second level. Operate on the lists inside of lists.</p>
<pre class="r"><code>library(purrr) #version 0.2.2
purrr::at_depth(dumbList,2,sum)
## $z
## $z$a
## [1] 3
## 
## $z$b
## [1] 5
## 
## 
## $y
## $y$a
## [1] 2
## 
## $y$b
## [1] 5
## 
## 
## $x
## $x$a
## [1] 4
## 
## $x$b
## [1] 2</code></pre>
<p>This is equivalent to the following code which requires an anonymous function to work.</p>
<pre class="r"><code>lapply(dumbList,function(x){
  lapply(x,sum)
})</code></pre>
<p>How does this relate to <code>allSubsetsRegression</code>? Let’s look at the results of running it.</p>
</div>
<div id="results-of-allsubetsregression" class="section level3">
<h3>Results of allSubetsRegression</h3>
<p>The function returns a hierarchical list - the top level has <span class="math inline">\(k\)</span> elements and the second level has <span class="math inline">\(i\)</span> choose <span class="math inline">\(k\)</span> elements.</p>
<pre class="r"><code>res = allSubsetsRegression(df,&quot;Y&quot;)
length(res) #4 because we had 4 independent variables
## [1] 4
lapply(res,length) # 4, 6, 4, 1 because there are 4 ways to select 1 variable from a group of 4, 6 ways to select 2, 4 ways to select 3 and 1 way to select 4
## [[1]]
## [1] 4
## 
## [[2]]
## [1] 6
## 
## [[3]]
## [1] 4
## 
## [[4]]
## [1] 1</code></pre>
<p>This allows me to easily index into any specific model via <code>res[[k]][[&quot;terms&quot;]]</code> and inspect that model. Want to see <code>X1+X2+X3</code>?</p>
<pre class="r"><code>res[[3]][[&quot;X1+X2+X3&quot;]]
## 
## Call:
## lm(formula = form, data = data)
## 
## Coefficients:
## (Intercept)           X1           X2           X3  
##     0.03424      0.97513      2.99673     -0.63855</code></pre>
<p>Now I want to calculate goodness statistics on each of these so I can determine the best models.</p>
</div>
<div id="reggoodness" class="section level3">
<h3>regGoodness</h3>
<p><code>regGoodness</code> (regression goodness) operates on the hierarchical list produced by <code>allSubsetsRegression</code>, applying a function which produces a ‘goodness’ score and then sorts by if that goodness score should be minimzed or maximized. It does this by using <code>purrr::at_depth</code> to allow a clean hierarchical output when generating the models, but ease of computing summary statistics afterward. <strong>note</strong> this does leverage the <code>stringr</code> package.</p>
<pre class="r"><code>library(stringr) #version 1.2.0

regGoodness = function(models,f,direction){
  if(!(direction %in% c(&quot;&gt;&quot;,&quot;&lt;&quot;))) stop(&quot;Invalid direction - must be &lt; or &gt;&quot;)
  if(!is.function(f)) stop(&quot;f must be a function&quot;)
  if(!(&quot;subReg&quot; %in% class(models))) stop(&quot;models must be generated by allSubsetsRegression&quot;)
  score = unlist(purrr::at_depth(models,2,f))
  output = data.frame(N = stringr::str_count(names(score),&quot;\\+&quot;)+1,Terms = names(score),Value = score,stringsAsFactors = FALSE)
  rownames(output) = NULL
  if(direction == &quot;&lt;&quot;){
    return(output[order(output$Value),])
  } else {
    return(output[order(output$Value,decreasing = T),])
  }
}</code></pre>
<p>For example, we could get the <span class="math inline">\(R^2_{Adj}\)</span> for each model by using this function.</p>
<pre class="r"><code>asrAR2 = function(model){
  summary(model)$adj.r.squared
}</code></pre>
<p>We pass it into the function thusly and get out a data.frame with summary details about each model fit.</p>
<pre class="r"><code>ar2 = regGoodness(res,asrAR2,&quot;&gt;&quot;)
ar2
##    N       Terms        Value
## 11 3    X1+X2+X3  0.804526215
## 15 4 X1+X2+X3+X4  0.790234207
## 5  2       X1+X2  0.771840082
## 12 3    X1+X2+X4  0.768256130
## 8  2       X2+X3  0.747873190
## 14 3    X2+X3+X4  0.727369827
## 2  1          X2  0.723193708
## 9  2       X2+X4  0.709493147
## 6  2       X1+X3  0.094866148
## 1  1          X1  0.055237747
## 7  2       X1+X4  0.048400898
## 13 3    X1+X3+X4  0.042796504
## 3  1          X3  0.003718353
## 4  1          X4 -0.013302437
## 10 2       X3+X4 -0.028144430</code></pre>
</div>
<div id="coefficient-stability" class="section level3">
<h3>Coefficient Stability</h3>
<p>We can also analyze how stable our coefficient estimates are as we vary what is in or out of the model. First we need to extract the coefficients from each model, then mutate the vector so it becomes a named data frame. The <code>dplyr::bind_rows</code> function is great because, like a “UNION ALL” query in SQL, keeps all the column names as it binds data frames together like new rows.</p>
<pre class="r"><code>library(dplyr) #version 0.5.0
coefs = purrr::at_depth(res,2,coefficients) %&gt;% 
  purrr::at_depth(2,function(x){as.data.frame(t(as.matrix(x)))}) %&gt;% 
  purrr::reduce(bind_rows) %&gt;% 
  mutate(Terms = ar2$Terms)
coefs
##     (Intercept)        X1       X2         X3          X4       Terms
## 1   0.150404089 0.8892705       NA         NA          NA    X1+X2+X3
## 2   0.031631652        NA 2.962518         NA          NA X1+X2+X3+X4
## 3   0.001309230        NA       NA -0.4622766          NA       X1+X2
## 4   0.017983923        NA       NA         NA  0.11390661    X1+X2+X4
## 5   0.051936938 0.8799054 2.880028         NA          NA       X2+X3
## 6   0.095013433 1.1728420       NA -0.4231423          NA    X2+X3+X4
## 7   0.047982016 0.9208031       NA         NA  0.16213820          X2
## 8   0.003984185        NA 3.093045 -0.6135762          NA       X2+X4
## 9   0.013940875        NA 2.881377         NA  0.02489288       X1+X3
## 10  0.060029856        NA       NA -0.2083143  0.11762346          X1
## 11  0.034239325 0.9751328 2.996726 -0.6385541          NA       X1+X4
## 12 -0.021320977 0.9166835 2.816281         NA  0.09154406    X1+X3+X4
## 13  0.079566784 1.0442746       NA -0.1011008  0.15250775          X3
## 14 -0.024519496        NA 2.986471 -0.5301225 -0.01590184          X4
## 15 -0.026595776 0.9885010 2.966883 -0.5368965  0.06566652       X3+X4</code></pre>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>All subsets regression presents many challenges, theoretical and practical. I was able to easily write some functional code which produced a hierarchical list. The structure is nice for exploring the results but presents hurdles when trying to analyze it with functional programming. <code>purrr::at_depth</code> allows for easy functional programming on hierarchical lists without the need for anonymous functions or other cumbersome overhead.</p>
</div>
</content:encoded>
    </item>
    
    <item>
      <title>Making this Blog</title>
      <link>http://thug-r.life/post/2017-08-08-making-the-blog/</link>
      <pubDate>Tue, 08 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>http://thug-r.life/post/2017-08-08-making-the-blog/</guid>
      <description>blogdown magicI made this blog using the magic of blogdown which is a framework that joins together a few technologies to make ‘static websites’
RMarkdownHugoRMarkdownRMarkdown is an interface from R to markdown which is a way of marking up a text document with formating so other tools can convert it from a text file to a pretty format (PDF, Word, etc.) It’s a great way of documenting your code by having your narrative right next to the code that generated the insights/results.</description>
      <content:encoded><div id="blogdown-magic" class="section level1">
<h1>blogdown magic</h1>
<p>I made this blog using the magic of <code>blogdown</code> which is a framework that joins together a few technologies to make ‘static websites’</p>
<ul>
<li>RMarkdown</li>
<li>Hugo</li>
</ul>
<div id="rmarkdown" class="section level2">
<h2>RMarkdown</h2>
<p>RMarkdown is an interface from R to markdown which is a way of marking up a text document with formating so other tools can convert it from a text file to a pretty format (PDF, Word, etc.) It’s a great way of documenting your code by having your narrative right next to the code that generated the insights/results.</p>
</div>
<div id="hugo" class="section level2">
<h2>Hugo</h2>
<p>Hugo is a static website generator written in Go. That’s all I know.</p>
<div id="static-website" class="section level3">
<h3>Static website?</h3>
<p>Most content on the web uses ‘server-side’ technology to render dynamic websites. It means that each time you access the same URL you could get different content based on who you are, where you’re from, etc. Contrast this with a ‘static site’ which is just a set of HTML files being served up. Granted - with Javascript you can do a lot of changing on the fly but that’s neither here nor there.</p>
<p>Wordpress as a blogging platform is an example of a dynamic website - each blog post is stored in a database and served from there.</p>
<p>Having a dynamic website means you need a server that can execute the code and serve up the pages. Provders like GoDaddy will sell you webhosting for $7/mo which is pretty cheap. In fact, for Wordpress, you’re essentially buying a SaaS solution (Software as a Solution) even though it’s really PaaS (Platform as a Service). You could buy a server from Digital Ocean, setup your own webserver and host it there. I actually ran servers out of my apartment when I was in college doing essentially this (granted, DO is Infrastructure as a Service where I was running bare metal). But then you have to administer the service which is…ugh. A pain.</p>
<p>But often times I like to do things differently not to necessarily save money but because they’re more novel. So, I wanted to host a static website with an Amazon S3 bucket because you can. And it’s supposed to be cheap. Pennies a month.</p>
</div>
</div>
<div id="domain-name" class="section level2">
<h2>Domain Name</h2>
<p>I bought my domain name from GoDaddy despite the myriad of ethical reasons not to. I’ve used them for years and while I know they’re evil, I already have an account and domains there.</p>
</div>
<div id="aws-s3" class="section level2">
<h2>AWS S3</h2>
<p>I already have an AWS account to manage my Lambda programs and hosting a bucket of pictures of miniature figures I scrape from Twitter so it wasn’t a big deal to make a few extra buckets.</p>
</div>
<div id="cloudflare" class="section level2">
<h2>Cloudflare</h2>
<p>I’m trying to use Cloudflare to manage my DNS and direct traffic from the domain name to the S3 bucket. This part isn’t going well right now.</p>
</div>
<div id="github" class="section level2">
<h2>GitHub</h2>
<p>I’m using GitHub as a…backup? It gives me a place to commit changes to and lets me work across multiple computers without missing a beat.</p>
</div>
<div id="travis-ci" class="section level2">
<h2>Travis CI</h2>
<p>Travis CI is a ‘continuous integration’ system which will build your code to test it each time you commit to GitHub. I’ve used it to test builds of my package <code>readOffice</code> as I’m developing it. Here, I’m using it to auto migrate new content to the S3 bucket.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>I’m stitching together a bunch of technologies that I’ve never/rarely used to make a blog. Which I’ve done lots of times using other solutions before.</p>
</div>
</div>
</content:encoded>
    </item>
    
    <item>
      <title>Theta Joins in R</title>
      <link>http://thug-r.life/post/2017-08-06-fuzzy-joining/</link>
      <pubDate>Sun, 06 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>http://thug-r.life/post/2017-08-06-fuzzy-joining/</guid>
      <description>Theta JoinsCommon Joins
If you’re like me, you’re looking at that diagram wondering what the heck is a ‘theta’ join?! Well, that’s the formal name for a ‘fuzzy’ join. In fact, you can read about all the different join types in this StackOverflow answer.
An equi join is the most common kind of join because it leverages keys in your data source which are essential in a SQL environment.</description>
      <content:encoded><div id="theta-joins" class="section level1">
<h1>Theta Joins</h1>
<div class="figure">
<img src="/img/2017-08-06-picture1.png" alt="Common Joins" width="400" />
<p class="caption">Common Joins</p>
</div>
<p>If you’re like me, you’re looking at that diagram wondering what the heck is a ‘theta’ join?! Well, that’s the formal name for a ‘fuzzy’ join. In fact, you can read about all the different join types <a href="https://stackoverflow.com/questions/7870155/difference-between-a-theta-join-equijoin-and-natural-join">in this StackOverflow answer</a>.</p>
<p>An equi join is the most common kind of join because it leverages keys in your data source which are essential in a SQL environment. In SQL it would be something like:</p>
<blockquote>
<p>select a.*, b.*<br />
from dbo.table1 as a<br />
left join dbo.table2 as b<br />
on a.key = b.key</p>
</blockquote>
<p>Actually this is a ‘natural’ join because the key has the same name in both sources and I’m using an equality to establish the join. But, because I’m joining based on two keys being the same, this fits into the ‘equi’ join category.</p>
<p>How can we do ‘equi’ joins in R? I’ll present three methods below using three different packages, <code>data.table</code>, <code>dplyr</code>, and <code>sqldf</code>.</p>
<div id="getting-ready" class="section level2">
<h2>Getting ready</h2>
<p>We’ll need to load some packages. Please note the versions I’m using. Also, the ‘live’ code I’m running is on my 64-bit Windows 7 laptop (8Gb RAM, i7-6600U CPU) in R 3.4.1</p>
<table>
<thead>
<tr class="header">
<th>Package</th>
<th>Version</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>data.table</td>
<td>1.10.4</td>
</tr>
<tr class="even">
<td>sqldf</td>
<td>0.4-10</td>
</tr>
<tr class="odd">
<td>dplyr</td>
<td>0.5.0</td>
</tr>
</tbody>
</table>
<pre class="r"><code>library(data.table)
library(sqldf)
library(dplyr)</code></pre>
<div id="equijoin-data" class="section level3">
<h3>equijoin data</h3>
<p>Two quick tables to demonstrate equijoin behavior in each ecosystem. Here we have some customer id’s in two tables, one which has purchases (df1) and another with their location. I’ll write everything in a function to enable easy benchmarking and profiling.</p>
<pre class="r"><code>df1 = data.frame(CustomerId = c(1:6), Product = c(rep(&quot;Laptop&quot;, 2), rep(&quot;Tablet&quot;, 2), rep(&quot;Smartphone&quot;, 2)))
df2 = data.frame(CustomerId = c(2,3,4,6), State = c(rep(&quot;Tennessee&quot;, 3), rep(&quot;North Carolina&quot;, 1)))</code></pre>
</div>
</div>
<div id="different-approaches-for-equijoins" class="section level2">
<h2>Different Approaches for Equijoins</h2>
<div id="base-r" class="section level3">
<h3>Base R</h3>
<pre class="r"><code>baseR_left = function(){
  merge(x = df1, y = df2, by = &quot;CustomerId&quot;, all.x = TRUE)
}
baseR_left()
##   CustomerId    Product          State
## 1          1     Laptop           &lt;NA&gt;
## 2          2     Laptop      Tennessee
## 3          3     Tablet      Tennessee
## 4          4     Tablet      Tennessee
## 5          5 Smartphone           &lt;NA&gt;
## 6          6 Smartphone North Carolina</code></pre>
</div>
<div id="data.table" class="section level3">
<h3>data.table</h3>
<p>Are you not familiar with <code>data.table</code>? I’ll be honest, I don’t use it a lot but it’s described as follows on CRAN:</p>
<blockquote>
<p>Fast aggregation of large data (e.g. 100GB in RAM), fast ordered joins, fast add/modify/delete of columns by group using no copies at all, list columns, a fast friendly file reader and parallel file writer. Offers a natural and flexible syntax, for faster development.</p>
</blockquote>
<p>The syntax is different from a lot of what I’m used to which is why I don’t use it a bunch, but that’s a bad reason to not consider it.</p>
<pre class="r"><code>dt1 = data.table(df1)
dt2 = data.table(df2)
#left join
dt_left = function(){
  dt2[dt1,on=.(CustomerId = CustomerId)]
}
dt_left()
##    CustomerId          State    Product
## 1:          1             NA     Laptop
## 2:          2      Tennessee     Laptop
## 3:          3      Tennessee     Tablet
## 4:          4      Tennessee     Tablet
## 5:          5             NA Smartphone
## 6:          6 North Carolina Smartphone</code></pre>
<p>I think this sytax is a bit backwards since Y[X] produces a left join of Y onto X meaning you put the left anchors on the right… anyway.</p>
</div>
<div id="sqldf" class="section level3">
<h3>sqldf</h3>
<p>Are you a lover of SQL and maybe new to R? I have a coworker who lived and breathed SQL for decades before learning R and they swear by this package to manage data. It literally sets up a database in the background to support the SQL language. That’s <em>dedication</em>.</p>
<blockquote>
<p>The sqldf() function is typically passed a single argument which is an SQL select statement where the table names are ordinary R data frame names. sqldf() transparently sets up a database, imports the data frames into that database, performs the SQL select or other statement and returns the result using a heuristic to determine which class to assign to each column of the returned data frame. The sqldf() or read.csv.sql() functions can also be used to read filtered files into R even if the original files are larger than R itself can handle. ‘RSQLite’, ‘RH2’, ‘RMySQL’ and ‘RPostgreSQL’ backends are supported.</p>
</blockquote>
<pre class="r"><code>sqldf_left = function(){
  sqldf(&quot;SELECT a.CustomerId, Product, State 
         FROM df1 a
         LEFT JOIN df2 b 
           on a.CustomerID = b.CustomerID&quot;)
}
sqldf_left()
##   CustomerId    Product          State
## 1          1     Laptop           &lt;NA&gt;
## 2          2     Laptop      Tennessee
## 3          3     Tablet      Tennessee
## 4          4     Tablet      Tennessee
## 5          5 Smartphone           &lt;NA&gt;
## 6          6 Smartphone North Carolina</code></pre>
</div>
<div id="dplyr" class="section level3">
<h3>dplyr</h3>
<p>If I described myself as having a man crush on Hadley Wickham it would be accurate. I have a signed copy of his book ‘Advanced R’ on my bookshelf which I treasure. I didn’t even get it signed, I sent it with a friend who went to an R conference where I knew Hadley would be present. ANYWAY… dplyr is part of the <a href="https://www.tidyverse.org/">tidyverse</a> which is a collection of packages to support ‘tidy’ data analysis. It includes support for many operations include joins.</p>
<pre class="r"><code>dplyr_left = function(){
  left_join(df1,df2,by=&quot;CustomerId&quot;)
}
dplyr_left()
##   CustomerId    Product          State
## 1          1     Laptop           &lt;NA&gt;
## 2          2     Laptop      Tennessee
## 3          3     Tablet      Tennessee
## 4          4     Tablet      Tennessee
## 5          5 Smartphone           &lt;NA&gt;
## 6          6 Smartphone North Carolina</code></pre>
</div>
</div>
<div id="profiling" class="section level2">
<h2>Profiling</h2>
<p>Let’s profile these just to get a sense of where they stand.</p>
<div id="profmem" class="section level3">
<h3>profmem</h3>
<table>
<thead>
<tr class="header">
<th>Ecosystem</th>
<th>profmem bytes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Base R</td>
<td>0</td>
</tr>
<tr class="even">
<td>data.table</td>
<td>58,048</td>
</tr>
<tr class="odd">
<td>sqldf</td>
<td>183,888</td>
</tr>
<tr class="even">
<td>dplyr</td>
<td>5,088</td>
</tr>
</tbody>
</table>
</div>
<div id="microbenchmark" class="section level3">
<h3>microbenchmark</h3>
<pre class="r"><code>library(microbenchmark)
microbenchmark(base = baseR_left(),
               dt = dt_left(),
               sqldf = sqldf_left(),
               dplyr = dplyr_left(),
               times = 100)
## Unit: microseconds
##   expr       min         lq      mean     median        uq       max neval
##   base   454.734   572.8845   727.651   627.5840   806.451  2958.868   100
##     dt   634.513   889.5935  1030.685   944.8395  1140.117  2257.259   100
##  sqldf 13749.219 15019.8840 17608.912 16303.4945 18693.672 67579.903   100
##  dplyr   940.464  1072.1070  1343.551  1134.6470  1511.889  5295.988   100</code></pre>
<div id="results" class="section level4">
<h4>Results</h4>
<p>Base R has the smallest memory footprint (duh) and performs pretty well. <code>merge</code> is pretty flexible and can do some cool stuff. dplyr has the fastest performance on this toy set and a small memory footprint. data.table takes about twice as long as dplyr and is ~10x the memory footprint. sqldf is slow and bloated - but what else could you expect from a package that is literally spinning up a database in the background? It’s the price you pay to use SQL in R.</p>
</div>
</div>
</div>
<div id="different-approaches-for-theta-joins" class="section level2">
<h2>Different Approaches for Theta Joins</h2>
</div>
<div id="the-small-data" class="section level2">
<h2>The (small) data</h2>
<p>We’ll use the same data for the toy problem and the more realistic version. This is the result of some text mining, specifically, named entity extraction from corpuses of text. It’s a two part, non-sequential process. We identify sentences by extracting the starting and ending characters.</p>
<pre class="r"><code>sent_df = read.table(text=&quot;id start end
                            1     1  39
                            2    42  63&quot;, header=TRUE)</code></pre>
<p>Then we extract entities by getting the type, starting and ending characters.</p>
<pre class="r"><code>ent_df = read.table(text=&quot;start end     ent_type
                              1   9 organization
                             23  31        money
                             45  50 organization&quot;, header=TRUE)</code></pre>
<p>But, to help our users get the context of where entities appear as well as to get co-occurence information, we need to know what sentence the entity appeared in. Additionally, we only want to keep the sentence id and the entity type.</p>
<p>In this case, we can see the first entity (an organization) starts at location 1 and ends at 9 which puts it in sentence 1 (which starts at location 1 and ends at 39). The second entity (money!) is also in sentence 1 (starts at 23 and ends at 31) while the last entity belongs in sentence 2. An equi join won’t work because I need to check if entity.start &gt;= sentence.start AND entity.end &lt;= sentence.end. Luckily, we have a few approaches.</p>
</div>
<div id="base-r-1" class="section level2">
<h2>Base R</h2>
<p>What we’ll do is cross join the data together, then do some filtering. I’m doing this all with base R functionality which I’m not totally used to doing, so this may not be the most perfect form of filtering. I decided to try using the <code>%between%</code> operator as well. Since an entity cannot be extracted which spans sentences all we really need to check is that the entity start is between the sentence start and end.</p>
<pre class="r"><code>baseR_fuzzy = function(){
  tmp = merge(x = sent_df, y = ent_df, by = NULL)
  tmp[tmp$start.x &lt;= tmp$start.y &amp; tmp$end.x &gt;= tmp$end.y,c(&quot;id&quot;,&quot;ent_type&quot;)]
}

baseR_fuzzy_between = function(){
  tmp = merge(x = sent_df, y = ent_df, by = NULL)
  tmp[tmp$start.y %between% tmp[,c(&quot;start.x&quot;,&quot;end.x&quot;)],c(&quot;id&quot;,&quot;ent_type&quot;)]
}
  
baseR_fuzzy()
##   id     ent_type
## 1  1 organization
## 3  1        money
## 6  2 organization</code></pre>
</div>
<div id="data.table-1" class="section level2">
<h2>data.table</h2>
<p>One great thing about data.table is that it supports fuzzy joins right out of the gate. However, there’s a lot of ambiguity in the join logic. Because doing a left join takes Y[X] and we’re joining the entities onto the sentences we list column names in the left-to-right order, not the left join order. So, where we have <code>on=.(start &gt;= start)</code> you can think of it as <code>on=.(start.Y &gt;= start.X)</code> which is equivalent to <code>on=.(start.ent &gt;= start.sent)</code>.</p>
<pre class="r"><code>sent_dt = data.table(sent_df)
ent_dt  = data.table(ent_df)

dt_fuzzy = function(){
  ent_dt[sent_dt, on=.(start &gt;= start, end &lt;= end)][,.(id, ent_type)]
}
dt_fuzzy()
##    id     ent_type
## 1:  1 organization
## 2:  1        money
## 3:  2 organization</code></pre>
</div>
<div id="sqldf-1" class="section level2">
<h2>sqldf</h2>
<p>sqldf also has the <code>BETWEEN</code> operator which lets us simplify the inequality of our join.</p>
<pre class="r"><code>sqldf_fuzzy = function() {
  sqldf(
      &quot;
      SELECT id, ent_type
      FROM sent_df a, ent_df b
      WHERE b.start BETWEEN a.start AND a.end 
      &quot;
  )
}
sqldf_fuzzy()
##   id     ent_type
## 1  1 organization
## 2  1        money
## 3  2 organization</code></pre>
</div>
<div id="fuzzyjoin" class="section level2">
<h2>fuzzyjoin</h2>
<p>Here I’ll introduce a new package aptly named <code>fuzzyjoin</code> (I’m using version 0.1.2). This adds ‘fuzzy’ versions of all the <code>dplyr</code> joins and is fully compatible in the <code>tidyverse</code>. In addition to providing both tables we need to provide match functions which provide logical indicators of fuzzy matching. What’s interesting is that each row is returned twice which must be a function of providing two match functions.</p>
<p>This is also the first time I use the <code>magrittr</code> pipes (%&gt;%). If you’re not familiar with them I’d suggest learning about them because they’re great and I have stickers of them on my truck and motorcycle.</p>
<pre class="r"><code>library(fuzzyjoin)
startMatch = function(x,y){
  x &lt;= y
}
endMatch = function(x,y){
  x &gt;= y
}
fuzzy_fuzzy = function(){
  fuzzy_join(sent_df,ent_df,by=c(&quot;start&quot;,&quot;end&quot;),match_fun = list(start = startMatch,end = endMatch)) %&gt;% 
    select(id,ent_type) %&gt;% 
    distinct()
}
fuzzy_fuzzy()
##   id     ent_type
## 1  1 organization
## 2  1        money
## 3  2 organization</code></pre>
</div>
<div id="pure-tidyverse" class="section level2">
<h2>Pure tidyverse</h2>
<p>I’m a fan of limiting the number of packages your code relies upon. Adding <code>fuzzyjoin</code> to handle this problem feels unnecessary (it provides great value in other applications because of the flexibility of the match functions) so I’d like to try and avoid it but also leverage the tidyverse. So I’m going to load the <code>tidyr</code> package (I’m using version 0.6.1) which is a solid compliment to <code>dplyr</code> and does lots of useful stuff. I know it looks like I’m just swapping out one package for another, but given the general practicality of tidyr compared to fuzzyjoin, I think it’s OK.</p>
<p>tidyr adds a cross_join function (not present in dplyr!!) which has a funky problem of not adjusting the names of duplicate columns. This impacts the ability of dplyr and other things to work on the resulting tibble (data.frame). So, I manually adjust the names using <code>names&lt;-</code>, filter down to what I want and then select the columns. The approach is identical to the base R code, just using the tidyverse.</p>
<pre class="r"><code>library(tidyr)
tidy_fuzzy = function(){
  sent_df %&gt;% 
    crossing(ent_df) %&gt;% 
    `names&lt;-`(c(&quot;id&quot;,&quot;start.x&quot;,&quot;end.x&quot;,&quot;start.y&quot;,&quot;end.y&quot;,&quot;ent_type&quot;)) %&gt;% 
    filter(end.x &gt;= end.y &amp; start.x &lt;= start.y) %&gt;% 
    select(id,ent_type)
}</code></pre>
</div>
<div id="profiling-1" class="section level2">
<h2>Profiling</h2>
<p>Let’s profile these (small) fuzzy joins just to get a sense of where they stand.</p>
<div id="profmem-1" class="section level3">
<h3>profmem</h3>
<table>
<thead>
<tr class="header">
<th>Ecosystem</th>
<th>profmem bytes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Base R</td>
<td>0</td>
</tr>
<tr class="even">
<td>Base R (%between%)</td>
<td>0</td>
</tr>
<tr class="odd">
<td>data.table</td>
<td>83,136</td>
</tr>
<tr class="even">
<td>sqldf</td>
<td>184,480</td>
</tr>
<tr class="odd">
<td>fuzzyjoin</td>
<td>117,104</td>
</tr>
<tr class="even">
<td>tidyverse</td>
<td>41,848</td>
</tr>
</tbody>
</table>
</div>
<div id="microbenchmark-1" class="section level3">
<h3>microbenchmark</h3>
<pre class="r"><code>microbenchmark(base = baseR_fuzzy(),
               base_between = baseR_fuzzy_between(),
               dt = dt_fuzzy(),
               sqldf = sqldf_fuzzy(),
               fuzzyjoin = fuzzy_fuzzy(),
               tidyverse = tidy_fuzzy(),
               times = 100)
## Unit: microseconds
##          expr        min          lq        mean      median         uq
##          base    420.821    503.4165    568.9828    550.0935    591.300
##  base_between    434.313    532.5900    681.3389    587.4715    631.413
##            dt   1203.750   1398.1150   1622.7250   1516.4475   1681.822
##         sqldf  13554.124  14266.3095  16467.4464  14979.7710  16349.442
##     fuzzyjoin 102653.099 110137.2410 120270.8023 113309.8015 122218.131
##     tidyverse  11972.950  12768.6420  13838.1710  13226.6585  14376.438
##         max neval
##    1261.367   100
##    6209.831   100
##    4577.968   100
##   58999.404   100
##  217263.831   100
##   26306.721   100</code></pre>
<div id="results-1" class="section level4">
<h4>Results</h4>
<p>Both base R functions have no memory footprint and actually run the fastest with the <code>%between%</code> function being a touch slower but not enough to ignore it’s improved readability. <code>fuzzyjoin</code> has a large footprint and runs the slowest being in the same ballpark as <code>sqldf</code> - neither of these are great choices for the task we have. <code>data.table</code> and <code>tidyverse</code> are in the same ballpark, but are substantially slower than base R. BUT this was a very small toy problem. Let’s do something more meaningful.</p>
</div>
</div>
</div>
<div id="bigger-data" class="section level2">
<h2>Bigger Data</h2>
<p>It turned out that in the real-world case we only needed to do fuzzy joining on a per-document basis which meant the data never got so big that scaling issues of each approach would matter. It was scaling how much parallel processing we could do to slam documents through.</p>
<p>But that’s boring. We have scaling issues we need to test and solve.</p>
<div class="figure">
<img src="/img/2017-08-06-picture2.png" alt="Common Joins" width="400" />
<p class="caption">Common Joins</p>
</div>
<p>Originally I created 5000 sentences with ~23,000 entities, but this proved to be too much for some of the functions to handle on my laptop (having closed all other applications first!). So, I shrunk it to 2000 sentences and ~9,000 entities.</p>
<pre class="r"><code>set.seed(2300)
nsent = 2000
sentence_length = floor(rnorm(nsent,100,10))
sent_df = data.frame(id = 1:nsent,
                     start = Reduce(sum,sentence_length,init=1,accumulate = T)[1:nsent],
                     end = cumsum(sentence_length))
stop = FALSE
i = 1
ent_df = data.frame(start = NA, end = NA, ent_type = NA)
while(!stop){
  ent_start = as.logical(rbinom(n=1,size=1,prob = 0.2))
  if(ent_start){
    sent = max(which(sent_df$start &lt;= i))
    sent_end = sent_df$end[sent_df$id == sent]
    ent_end = min(c(i+rpois(1,1)+3,sent_end))
    if(ent_end-i &lt; 3){
      i = i+3
      next
    }
    ent_type = sample(c(&quot;organization&quot;,&quot;person&quot;,&quot;money&quot;),size = 1)
    ent_df = rbind(ent_df,data.frame(start=i, end=ent_end, ent_type=ent_type))
    i = ent_end+2
  }
  i = i+3
  if(i &gt; max(sent_df$end)) stop = TRUE
}</code></pre>
<p>Now we have a bunch of data - so let’s test stuff!</p>
<div id="microbenchmark-2" class="section level3">
<h3>microbenchmark</h3>
<p>I suspect some of these are going to run very poorly, so we’ll start with an n of 1. This isn’t a good benchmark but when you see the eval times, you’ll realize why.</p>
<pre class="r"><code>microbenchmark(base = baseR_fuzzy(),
               base_between = baseR_fuzzy_between(),
               dt = dt_fuzzy(),
               sqldf = sqldf_fuzzy(),
               fuzzyjoin = fuzzy_fuzzy(),
               tidyverse = tidy_fuzzy(),
               times = 1)</code></pre>
<table>
<thead>
<tr class="header">
<th>Method</th>
<th>Eval Time (ms)</th>
<th>Scale Up</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Base</td>
<td>42,564</td>
<td>67,669</td>
</tr>
<tr class="even">
<td>Base (%between%)</td>
<td>53,239</td>
<td>81,280x</td>
</tr>
<tr class="odd">
<td>data.table</td>
<td>17</td>
<td>9x</td>
</tr>
<tr class="even">
<td>sqldf</td>
<td>1,296</td>
<td>798x</td>
</tr>
<tr class="odd">
<td>fuzzyjoin</td>
<td>355,983</td>
<td>16,100x</td>
</tr>
<tr class="even">
<td>tidyverse</td>
<td>65,113</td>
<td>38,301x</td>
</tr>
</tbody>
</table>
<p>The ‘scale up’ column is simply dividing the execution time of the 1000x sentence (3000x entities) data by the execution time of the original data. I find the results to be somewhat surprising.</p>
<div id="results-2" class="section level4">
<h4>Results</h4>
<ul>
<li><code>data.table</code> is the clear winner overall - it was middle of the pack on the toy data and scales better than the data.</li>
<li><code>sqldf</code> has a lot of overhead and ran (relatively) poorly on the toy problem but scales very well with the larger data.</li>
<li><code>fuzzyjoin</code> took nearly 6 minutes to run where the next slowest was closer to a minute. This is not a good choice for the type of fuzzy joining we’re doing.</li>
<li>Base R (both versions) and <code>tidyverse</code> were in the same rough ballpark, ~1 minute with ridiculous scaling problems.</li>
</ul>
</div>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>I’ve shown you 6 different approaches to doing fuzzy joins (aka theta joins) and demonstrated their relative merits. We’ve seen that performance on small joins is no indication of performance on large joins but <code>data.table</code> scales very well on a variety of sizes and <code>sqldf</code> leverages it’s bloat in small sizes to perform well on larger sizes. Also important to note - base R has reasonable performance and beats a ‘tidy’ solution at the (potential) cost of readability.</p>
<p>Should you pick up <code>data.table</code> and replace all your tidy work with it? I’m begining to believe that maybe I should. <code>data.table</code> has at least one other practical advantage over the tidyverse. Stability. <code>data.table</code> is past version 1.0 where <code>dplyr</code> and <code>tidyr</code> are not. This is especially apparent with the fairly frequent breaking changes made to the tidyverse which can have a strong, negative impact in an enterprise environment.</p>
<p>Well - I hope you have fun with theta joins (and start calling them that!)</p>
</div>
</div>
</content:encoded>
    </item>
    
  </channel>
</rss>